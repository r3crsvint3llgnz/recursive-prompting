# Results

## Results of My Self-Study

In this section, I share what I observed in my exploratory self-analysis of 441 of my own conversation threads (1,980 moves across 13 move types). The numbers and observations presented here describe patterns I noticed in my practice, using my own effectiveness scoring rubric. They are intended as teaching tools and hypotheses—not as universal or statistically validated proof.

### Observed Effectiveness

Based on my personal scoring rubric (0–20 scale, weighing relevance, completeness, and actionability), I estimated that recursive prompting improved outcomes by roughly 34% compared to my attempts at single-shot prompting. This estimate is entirely subjective and comes from my judgment as the practitioner, not from controlled experiments. Still, it gives me confidence that the recursive approach was noticeably more effective for me.

**Note:** Progression metrics in this section are based on the 370-conversation subset with complete sequences.

Across different dimensions, I noticed the following relative improvements in my personal experience:

- Response Completeness: ~42% increase in how fully the AI covered requirements (personal experience)
- Solution Quality: ~28% improvement in appropriateness of responses (personal experience)
- User Satisfaction (my own sense of usefulness): ~36% increase (personal experience)
- Goal Achievement: ~31% higher rate of reaching the objectives I set (personal experience)

These percentages reflect my subjective assessment of this dataset only and should not be treated as statistically significant or generalizable findings.

### Which Moves Helped Me Most

Certain types of moves stood out in my sessions:

- **Metacognitive Reflection**: When I stopped to critique assumptions, I often got much stronger results.
- **Systematic Decomposition**: Breaking down big problems into smaller components made conversations more manageable and productive.
- **Iterative Refinement**: Asking the AI to progressively improve drafts helped polish them into usable outputs.

Other moves were effective depending on the situation:

- **Hypothesis Testing**: Very useful in analytical contexts where I needed to test a theory.
- **Role-Based Prompting**: Helpful in strategy and creative tasks when I needed a new perspective.
- **Constraint Modification**: Gave me tighter, more usable outputs when I added requirements carefully.

### Conversation Dynamics I Observed

When my recursive prompting went well, I noticed certain dynamics in my own conversations:

- **Progressive Refinement**: Gradually converging on stronger answers through multiple cycles.
- **Exploratory Phases**: Using open-ended probing early, followed by narrowing in later.
- **Validation Loops**: Pausing to check and adjust before moving forward.

### Domain-Specific Patterns in My Practice

Although my data came from many types of conversations, I saw some domain-specific tendencies:

- **Technical Work**: I leaned on decomposition and testing sequences more heavily.
- **Creative Work**: I used more exploration and alternative-generation moves.
- **Analytical Work**: I relied more on reflection and validation cycles.

### Important Caveats

- This is a single-practitioner (n=1) self-study.
- All effectiveness scores come from my own rubric and judgment, not from objective benchmarks.
- Percentages are descriptive of my dataset only and should not be treated as statistically significant or generalizable.

### How I Use These Findings

For me, this analysis clarified which prompting strategies felt most useful and repeatable. I now use these results as the foundation for teaching recursive prompting: showing others the patterns I identified, while being explicit that they should test, adapt, and refine these approaches in their own practice.
