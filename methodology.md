# Methodology

## My Methodology

This section describes how I studied my own prompting practice. This was an exploratory, single-practitioner analysis (n=1) based on 441 of my conversations with ChatGPT, which contained 1,980 moves across 13 move types. My aim was to understand and document my own habits well enough to create teaching materials and hypotheses for others—not to conduct a controlled or generalizable study.

### Data Collection

The dataset came entirely from my own conversation history. These included a variety of tasks I worked on: research, creative writing, problem-solving, and educational content creation. I extracted these conversations from my logs and organized them into threads, each representing one interaction flow.

**Steps I took to prepare the data:**  
1. Segmented conversations into threads  
2. Broke each thread into individual prompt-response pairs  
3. Normalized text (removed artifacts, aligned formatting)  
4. Preserved context and ordering so I could see how moves flowed  
5. Filtered out incomplete or trivial threads  

### Move Classification

I developed a taxonomy of 13 move types based on what I observed in my own prompting. Examples include Probe, ScopeTighten, Refine, RoleShift, Meta, and Hypothesis. Each prompt I made was manually classified by me into one of these categories.

**Important biases:** I did all classification myself without independent validation. This means the labels reflect my own interpretation and include subjective bias. Additionally, I tended to analyze conversations I considered "successful," introducing selection bias into the dataset.

### Effectiveness Scoring

I also scored many of the AI's responses using a personal rubric (0–20 scale). My rubric combined:  
- Relevance (30%)  
- Completeness (20%)  
- Actionability (10%)  
- Response length as a rough proxy (40%)  

This was entirely subjective. No other raters or objective benchmarks were used. The scores represent how useful the responses felt to me at the time.

### Analytical Approach

**Progression Metrics Coverage.** While the full dataset includes 441 conversations, progression metrics were computed for 370 conversations where move sequences were sufficiently complete to evaluate progression-level indicators. Counts elsewhere in this paper (e.g., total conversations, total moves, move types, patterns, templates) refer to the full dataset; progression-specific summaries refer to this 370-conversation subset.

To make sense of my data, I explored several descriptive analyses:

- Frequency analysis: How often I used each move type  
- Transition analysis: Which moves tended to follow each other  
- Sequence analysis: Recurring move sequences across conversations  
- Pattern extraction: Identifying repeated move structures that I could formalize as templates  

These analyses were tools for reflection. They helped me see habits I wasn't always aware of during live conversations.

### Template Generation

From the recurring sequences I observed, I distilled 20 recurring patterns distilled into 16 reusable templates (v1) (see Pattern Templates (v1)). Each template includes:  
- The move sequence  
- Contexts where I found it useful  
- Example prompts from my own practice  
- Notes about strengths, weaknesses, and adaptations  

### Methodological Limitations

Because this was a self-study, there are clear limitations:  
- **Single-practitioner (n=1)**: Only my own data was used  
- **Subjective scoring**: Effectiveness was based on my personal rubric (~34% overall effectiveness estimate)  
- **Self-classification**: All move labeling was done by me alone  
- **Selection bias**: I tended to analyze conversations I considered "successful"  
- **No control group**: I did not compare directly with other users or single-shot baselines  

### Summary

This methodology reflects how I examined my own prompting practice. It provided enough structure for me to identify patterns, build a library of templates, and generate hypotheses that I can now teach and share. However, it does not provide statistical proof or generalizable claims. Future multi-user controlled studies will be necessary to test whether the patterns I observed hold true more broadly.
