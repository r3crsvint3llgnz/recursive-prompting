# Recursive Prompting: An Exploratory Self-Study

## Abstract
(From abstract.md)
This white paper documents my exploratory, single-practitioner analysis (n=1) of **recursive prompting** based on **441** of my own conversation threads comprising **1,980** prompting moves across **13** move types. Through this self-analysis, I identified **20** recurring sequence patterns and formalized **16** reusable templates (v1) across five categories—**Exploration**, **Refinement**, **Metacognitive**, **Iterative Development**, and **Structured Problem-Solving**.

My goal in this project was not to prove general effectiveness, but to understand my own prompting techniques well enough to teach them to others. The numbers and structures presented here come from patterns I observed in my own sessions with ChatGPT. They should be treated as **hypotheses and teaching tools**, not definitive research findings.

---

## Introduction
(From introduction.md)
This work represents my exploratory self-study of recursive prompting. Rather than a controlled or generalizable research project, this is my attempt to analyze and formalize my own practice so I can teach it to others. My aims were to characterize the moves I used, document how they worked for me, distill recurring sequences into templates, and generate hypotheses others can test.

---

## Methodology
(From methodology.md)
I analyzed 441 of my own conversations (1,980 moves across 13 move types). I manually classified each move, scored responses with a personal rubric, and identified recurring sequences. From these I distilled 20 patterns into 16 reusable templates (v1). All classification and scoring was subjective, reflecting only my own practice.

---

## Move Analysis
(From move_analysis.md)
My most common moves were Probe, ScopeTighten, ScopeLoosen, and FormatControl. Less frequent but often disproportionately helpful were Meta (reflection), Decompose, and Hypothesis. Certain transitions became my recurring habits, such as Probe → Refine, Seed → Probe, and ScopeTighten ↔ ScopeLoosen. These habits later formed the backbone of my template library.

---

## Pattern Analysis
(From pattern_analysis.md)
From my dataset, I grouped patterns into five categories: Exploration, Refinement, Metacognitive, Iterative Development, and Structured Problem-Solving. These categories helped me organize when and why I used certain sequences. From them I formalized 20 patterns into 16 reusable templates (v1). Even though all my data came from my own practice, I found these patterns applied across technical work, creative writing, research, education, and strategic planning.

---

## Pattern Templates (v1)
(See template library)
Sixteen reusable templates with move sequences, example prompts, contexts, and notes from my practice.

---

## Results
(From results.md)
Based on my scoring rubric, recursive prompting improved outcomes for me by about 34% compared to single-shot prompting. Completeness improved ~42%, solution quality ~28%, satisfaction ~36%, and goal achievement ~31%. These numbers are descriptive of my experience only. I found metacognition, decomposition, and refinement especially helpful.

---

## Effectiveness Analysis
(From effectiveness_analysis.md)
I found effectiveness depended on strategic move selection, adaptive sequencing, and metacognitive awareness. Failure modes included pattern mismatch, premature convergence, neglecting reflection, and context loss. I optimized by building a pattern portfolio, matching patterns to context, and reflecting more deliberately.

---

## Discussion
(From discussion.md)
Recursive prompting felt cognitively natural to me, aligning with metacognitive regulation, elaborative processing, decomposition, and hypothesis testing. My results suggest opportunities for tools, training, and research that scaffold recursive prompting. Limitations remain (n=1, subjective scoring), but I see potential for education, democratic access, research acceleration, and creativity.

---

## Differentiating Recursive Prompting from Mainstream Techniques

Most prompting guides from OpenAI, Anthropic, and Google focus on **single-shot** and **few-shot** techniques: provide a well-structured prompt or a few examples, and the model generates a one-pass output. These are valuable, but in my experience they often fall short for complex, multi-step tasks.

Recursive prompting adds a **conversational funnel** approach:

- **Start broad** (Probe, Seed) to generate options or raw material.
- **Tighten scope** (ScopeTighten, ConstraintsAdd) to narrow in on promising directions.
- **Reflect** (Meta) to check assumptions and alignment with goals.
- **Expand if needed** (ScopeLoosen, RoleShift) to bring in new perspectives or adjacent considerations.
- **Refine** to polish outputs iteratively until a usable result emerges.

### Example Comparison

**Single-Shot Prompt:**  
“Write me a 1-page lesson plan on climate change for middle school students.”  
→ Produces a generic lesson plan, sometimes misaligned with my goals.

**Recursive Prompting (Probe → ScopeTighten → Refine):**  
1. Seed: “Draft a simple lesson plan on climate change.”  
2. Probe: “Give me 3 variations emphasizing science, policy, and personal action.”  
3. ScopeTighten: “Focus on the science version, but align it with state curriculum standards.”  
4. Refine: “Polish the lesson to fit a 45-minute class, with a student activity and a quiz.”  

→ Produces a tailored, structured lesson plan that is more useful and actionable.

### Branching and Forking

Recursive prompting also allows **branching**: exploring multiple variations in parallel. If one branch fails, I can fork and refine another, rather than starting from scratch. This is something mainstream prompting guides rarely emphasize.

---

## Implications for Agentic Capabilities

Recursive prompting provides a natural foundation for more advanced **agentic AI systems**:

- **Enhanced AI UIs**: Interfaces that visualize branches, manage forks, and surface recursive templates could make this process smoother.  
- **Multi-User Collaboration**: Teams could coordinate by assigning different branches of a recursive funnel, then converging on a shared solution.  
- **Agent Strategies**: Recursive prompting patterns could be encoded as strategies for autonomous agents, guiding them through exploration, reflection, and refinement cycles.  
- **Complex Problem-Solving**: The funnel approach of recursive prompting—zooming in and out, reflecting, refining—could enable agents to tackle layered, ambiguous challenges more effectively than single-pass prompting.

---

## Conclusion
(From conclusion.md)
For me, recursive prompting is more than a set of techniques—it is a way of working with AI that mirrors how I think and learn. By reflecting, iterating, and structuring conversations, I get better results than with one-off prompts. My role here has been to document what I found in my own sessions and to offer it as a stepping stone for others. Recursive prompting will evolve as more people practice it, study it, and build tools around it.

---

## Context and Further Reading
(From references.md)
This study was based entirely on my own analysis. The works listed here are provided as context and further reading, not as sources of my results. They include foundational prompting research (Chain-of-Thought, Prompt Pattern Catalog, Prompt Report) and practitioner guides from OpenAI, Google, and Anthropic. They represent the broader field I hope to connect with in future studies.
